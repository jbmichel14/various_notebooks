{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet allocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.clustering.LDA.html#pyspark.mllib.clustering.LDA\n",
    "\n",
    "https://spark.apache.org/docs/1.6.2/mllib-clustering.html#latent-dirichlet-allocation-lda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Topics extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processed courses: list of documents with their word-count vector (represented as a dict{word:count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ppcourses.pkl', 'rb') as f:\n",
    "    pre_processed_courses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_processed_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of words: the set of all occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/set_of_words.pkl', 'rb') as f1:\n",
    "    set_of_words = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_words = list(set_of_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5806"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(pre_processed_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of count vectors for each document (shape: 854x3002)\n",
    "parsedData = data.map(lambda d: Vectors.dense([d['description'][w] for w in set_of_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create an id for each document\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster the documents into 10 topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#topics matrix: for every 5806 word, we obtain a size-10 vector with the word's distribution per topic\n",
    "topics = ldaModel.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prints the top-m words for the first l topics\n",
    "def print_topics_top_words(ldaModel, topics, l, m):\n",
    "    for topic in range(l):\n",
    "        print(\"- Topic \" + str(topic+1) + \":\")\n",
    "        w = [topics[word][topic] for word in range(0, ldaModel.vocabSize())]\n",
    "        x = np.flip(np.argsort(w))\n",
    "        for i in range(0,m):\n",
    "            print(set_of_words[x[i]])\n",
    "        print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Topic 1:\n",
      "drug\n",
      "cancer\n",
      "business\n",
      "doctoral\n",
      "disease\n",
      "open\n",
      "area\n",
      "priority\n",
      "pathway\n",
      "doctoral student\n",
      "----------------------\n",
      "-Topic 2:\n",
      "signal processing\n",
      "matlab\n",
      "linear algebra\n",
      "finite\n",
      "fluid\n",
      "regression\n",
      "estimation\n",
      "discrete\n",
      "speech\n",
      "coding\n",
      "----------------------\n",
      "-Topic 3:\n",
      "building\n",
      "urban\n",
      "structural\n",
      "studio\n",
      "reading\n",
      "identify\n",
      "beam\n",
      "thinking\n",
      "architectural\n",
      "making\n",
      "----------------------\n",
      "-Topic 4:\n",
      "noise\n",
      "low\n",
      "cmos\n",
      "analog\n",
      "wave\n",
      "acoustic\n",
      "fiber\n",
      "coupling\n",
      "mode\n",
      "filter\n",
      "----------------------\n",
      "-Topic 5:\n",
      "financial\n",
      "chain\n",
      "pricing\n",
      "finance\n",
      "option\n",
      "portfolio\n",
      "choice\n",
      "asset\n",
      "fracture\n",
      "supply\n",
      "----------------------\n",
      "-Topic 6:\n",
      "treatment\n",
      "robot\n",
      "c\n",
      "quality\n",
      "production\n",
      "plasma\n",
      "object\n",
      "wastewater\n",
      "animal\n",
      "student learn\n",
      "----------------------\n",
      "-Topic 7:\n",
      "policy\n",
      "conversion\n",
      "social\n",
      "thermodynamic\n",
      "rate\n",
      "equilibrium\n",
      "energy conversion\n",
      "medium\n",
      "cycle\n",
      "balance\n",
      "----------------------\n",
      "-Topic 8:\n",
      "progress\n",
      "written report\n",
      "neuroscience\n",
      "acquired\n",
      "waste\n",
      "adapt\n",
      "progress plan\n",
      "plan adapt\n",
      "ass progress\n",
      "adapt plan\n",
      "----------------------\n",
      "-Topic 9:\n",
      "theorem\n",
      "calculus\n",
      "common\n",
      "en\n",
      "integral\n",
      "spectroscopy\n",
      "la\n",
      "convergence\n",
      "ressources\n",
      "public\n",
      "----------------------\n",
      "-Topic 10:\n",
      "metal\n",
      "semiconductor\n",
      "reactor\n",
      "force\n",
      "molecule\n",
      "thermal\n",
      "liquid\n",
      "film\n",
      "electronics\n",
      "kinetics\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print_topics_top_words(ldaModel, topics, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "Some labels for each topic:\n",
    "- Topic 1: Pharma\n",
    "- Topic 2: Signal Processing \n",
    "- Topic 3: Civil Engineering\n",
    "- Topic 4: Electronics\n",
    "- Topic 5: Business Administration/Economics\n",
    "- Topic 6: Various Subjects\n",
    "- Topic 7: SHS/Thermodynamics \n",
    "- Topic 8: Study Methods\n",
    "- Topic 9: Maths\n",
    "- Topic 10: Materials Engineering\n",
    "\n",
    "We may obtain a different result everytime we train the model.\n",
    "Some topics are hard to describe (like 6 and 7) as they seem to cover several subjects.\n",
    "\n",
    "LDA seems to perform better than LSI here, with more clear and better separated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PySpark library, this is how the hyperparameters are called\n",
    "- $\\alpha$ : docConcentration\n",
    "- $\\beta$ : topicConcentration\n",
    "\n",
    "$\\alpha$ is the distribution of topics in documents and $\\beta$ is the distributions of words over topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trains a LDA model with hyperparameters alpha, beta and k, prints the top words of first few topics\n",
    "def LDA_with_hyperparameters(alpha, beta, k=10):\n",
    "    ldaModel = LDA.train(corpus, k=10, docConcentration=alpha, topicConcentration=beta, seed=0)\n",
    "    topics = ldaModel.topicsMatrix()\n",
    "    print_topics_top_words(ldaModel, topics, l=3, m=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default values:\n",
    "alpha = 6.0\n",
    "beta = 1.01\n",
    "#test values:\n",
    "alphas = [1.01, 2.0, 6.0, 10.0, 50.0, 100.0, 500.0]\n",
    "betas = [1.01, 1.51, 3.0, 6.0, 10.0, 50.0, 100.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of test-values is based on the fact that:\n",
    "-  a high alpha-value means that each document is likely to contain a mixture of most of the topics, and not any single topic specifically.\n",
    "- a high beta-value means that each topic is likely to contain a mixture of most of the words, and not any word specifically, while a low value means that a topic may contain a mixture of just a few of the words.\n",
    "\n",
    "\n",
    "(Note: This implementation of LDA requires alpha > 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### alpha = 1.01 ####\n",
      "-Topic 1:\n",
      "low\n",
      "analog\n",
      "noise\n",
      "----------------------\n",
      "-Topic 2:\n",
      "security\n",
      "finance\n",
      "final exam\n",
      "----------------------\n",
      "-Topic 3:\n",
      "supply\n",
      "culture\n",
      "grid\n",
      "----------------------\n",
      "#### alpha = 2.0 ####\n",
      "-Topic 1:\n",
      "noise\n",
      "low\n",
      "analog\n",
      "----------------------\n",
      "-Topic 2:\n",
      "finance\n",
      "security\n",
      "final exam\n",
      "----------------------\n",
      "-Topic 3:\n",
      "culture\n",
      "supply\n",
      "chain\n",
      "----------------------\n",
      "#### alpha = 6.0 ####\n",
      "-Topic 1:\n",
      "noise\n",
      "low\n",
      "finite\n",
      "----------------------\n",
      "-Topic 2:\n",
      "final exam\n",
      "discrete\n",
      "finance\n",
      "----------------------\n",
      "-Topic 3:\n",
      "culture\n",
      "supply\n",
      "fluid\n",
      "----------------------\n",
      "#### alpha = 10.0 ####\n",
      "-Topic 1:\n",
      "noise\n",
      "analog\n",
      "low\n",
      "----------------------\n",
      "-Topic 2:\n",
      "security\n",
      "discrete\n",
      "calculus\n",
      "----------------------\n",
      "-Topic 3:\n",
      "fluid\n",
      "chain\n",
      "supply\n",
      "----------------------\n",
      "#### alpha = 50.0 ####\n",
      "-Topic 1:\n",
      "business\n",
      "financial\n",
      "theorem\n",
      "----------------------\n",
      "-Topic 2:\n",
      "speech\n",
      "food\n",
      "option\n",
      "----------------------\n",
      "-Topic 3:\n",
      "snow\n",
      "electronics\n",
      "chain\n",
      "----------------------\n",
      "#### alpha = 100.0 ####\n",
      "-Topic 1:\n",
      "business\n",
      "production\n",
      "metal\n",
      "----------------------\n",
      "-Topic 2:\n",
      "distribution\n",
      "spectroscopy\n",
      "speech\n",
      "----------------------\n",
      "-Topic 3:\n",
      "electronics\n",
      "industry\n",
      "demonstrate\n",
      "----------------------\n",
      "#### alpha = 500.0 ####\n",
      "-Topic 1:\n",
      "business\n",
      "production\n",
      "metal\n",
      "----------------------\n",
      "-Topic 2:\n",
      "spectroscopy\n",
      "distribution\n",
      "discrete\n",
      "----------------------\n",
      "-Topic 3:\n",
      "electronics\n",
      "industry\n",
      "quality\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "#1. Vary alpha\n",
    "for a in alphas:\n",
    "    print(\"#### alpha = \" + str(a) + \" ####\")\n",
    "    LDA_with_hyperparameters(a, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: \n",
    "\n",
    "Varying $\\alpha$ by a little bit (between 1 and 10) does not have much effect. It starts having an effect when $\\alpha$ starts getting bigger: as we jump to $\\alpha = 50$, we see a big change in every topic. Then, the result tends to stabilize (even by making $\\alpha$ change a lot), with small changes but the same top-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### beta = 1.01 ####\n",
      "-Topic 1:\n",
      "noise\n",
      "low\n",
      "finite\n",
      "----------------------\n",
      "-Topic 2:\n",
      "final exam\n",
      "discrete\n",
      "finance\n",
      "----------------------\n",
      "-Topic 3:\n",
      "culture\n",
      "supply\n",
      "fluid\n",
      "----------------------\n",
      "#### beta = 1.51 ####\n",
      "-Topic 1:\n",
      "low\n",
      "noise\n",
      "analog\n",
      "----------------------\n",
      "-Topic 2:\n",
      "financial\n",
      "finance\n",
      "security\n",
      "----------------------\n",
      "-Topic 3:\n",
      "chain\n",
      "supply\n",
      "sample\n",
      "----------------------\n",
      "#### beta = 3.0 ####\n",
      "-Topic 1:\n",
      "discrete\n",
      "business\n",
      "multi\n",
      "----------------------\n",
      "-Topic 2:\n",
      "speech\n",
      "signal processing\n",
      "discrete\n",
      "----------------------\n",
      "-Topic 3:\n",
      "snow\n",
      "chain\n",
      "electronics\n",
      "----------------------\n",
      "#### beta = 6.0 ####\n",
      "-Topic 1:\n",
      "large\n",
      "final exam\n",
      "reading\n",
      "----------------------\n",
      "-Topic 2:\n",
      "large\n",
      "quality\n",
      "final exam\n",
      "----------------------\n",
      "-Topic 3:\n",
      "reading\n",
      "quality\n",
      "large\n",
      "----------------------\n",
      "#### beta = 10.0 ####\n",
      "-Topic 1:\n",
      "large\n",
      "quality\n",
      "reading\n",
      "----------------------\n",
      "-Topic 2:\n",
      "large\n",
      "quality\n",
      "reading\n",
      "----------------------\n",
      "-Topic 3:\n",
      "reading\n",
      "quality\n",
      "large\n",
      "----------------------\n",
      "#### beta = 50.0 ####\n",
      "-Topic 1:\n",
      "reading\n",
      "large\n",
      "quality\n",
      "----------------------\n",
      "-Topic 2:\n",
      "large\n",
      "quality\n",
      "reading\n",
      "----------------------\n",
      "-Topic 3:\n",
      "reading\n",
      "quality\n",
      "large\n",
      "----------------------\n",
      "#### beta = 100.0 ####\n",
      "-Topic 1:\n",
      "reading\n",
      "quality\n",
      "large\n",
      "----------------------\n",
      "-Topic 2:\n",
      "large\n",
      "quality\n",
      "reading\n",
      "----------------------\n",
      "-Topic 3:\n",
      "reading\n",
      "quality\n",
      "large\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "#2. Vary beta\n",
    "for b in betas:\n",
    "    print(\"#### beta = \" + str(b) + \" ####\")\n",
    "    LDA_with_hyperparameters(alpha, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "On the other hand, small variations on a small $\\beta$, even by 1/2 has an effect on the topics' _leading_ words, event though the top words roughly stay the same. We observe a lot of change with values $\\beta = \\{1.01, 1.51, 3.0,  6.0\\}$. But from 6.0, and all larger $\\beta's$, all topics start containing the same top-words, which is expected as it means all topics are more likely to contain a mixture of all words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 7 faculties and more or less 15 sections. Our corpus has 854 documents and 5806 words.\n",
    "\n",
    "In our case, every document describes a specific course so is more likely to have one or a few topics. And as each topic seems to be some scientific subject, a topic may contain a mixture of just a few words.\n",
    "\n",
    "\n",
    "\n",
    "- k: They are many faculties and different fields of engineering taught at EPFL, so we pick a rather large k to make sure topics are well separated. We also want to avoid a topic containing several unrelated subjects.\n",
    "- alpha: rather small because each document contain a small number of topics.\n",
    "- beta: small because it will produce quite specific topics when it comes to words, as we chose a rather large k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 6.0\n",
    "beta = 1.01\n",
    "k = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModelEPFL = LDA.train(corpus, k, docConcentration=alpha, topicConcentration=beta, seed=2)\n",
    "topics = ldaModelEPFL.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Topic 1:\n",
      "fluid\n",
      "layer\n",
      "thermodynamic\n",
      "conversion\n",
      "bio\n",
      "----------------------\n",
      "-Topic 2:\n",
      "electronics\n",
      "low\n",
      "analog\n",
      "brain\n",
      "wireless\n",
      "----------------------\n",
      "-Topic 3:\n",
      "progress\n",
      "written report\n",
      "acquired\n",
      "obtained\n",
      "professor\n",
      "----------------------\n",
      "-Topic 4:\n",
      "reactor\n",
      "kinetics\n",
      "equilibrium\n",
      "law\n",
      "liquid\n",
      "----------------------\n",
      "-Topic 5:\n",
      "chain\n",
      "business\n",
      "supply\n",
      "regression\n",
      "urban\n",
      "----------------------\n",
      "-Topic 6:\n",
      "financial\n",
      "calculus\n",
      "rate\n",
      "finance\n",
      "option\n",
      "----------------------\n",
      "-Topic 7:\n",
      "policy\n",
      "industry\n",
      "coding\n",
      "speech\n",
      "recognition\n",
      "----------------------\n",
      "-Topic 8:\n",
      "quality\n",
      "waste\n",
      "discipline\n",
      "reading\n",
      "sample\n",
      "----------------------\n",
      "-Topic 9:\n",
      "cycle\n",
      "membrane\n",
      "conversion\n",
      "corporate\n",
      "energy conversion\n",
      "----------------------\n",
      "-Topic 10:\n",
      "drug\n",
      "doctoral\n",
      "selected\n",
      "disease\n",
      "food\n",
      "----------------------\n",
      "-Topic 11:\n",
      "robot\n",
      "reach\n",
      "production\n",
      "set objective\n",
      "action plan\n",
      "----------------------\n",
      "-Topic 12:\n",
      "spectroscopy\n",
      "tissue\n",
      "diffraction\n",
      "ray\n",
      "studio\n",
      "----------------------\n",
      "-Topic 13:\n",
      "semiconductor\n",
      "fiber\n",
      "3d\n",
      "stress\n",
      "fracture\n",
      "----------------------\n",
      "-Topic 14:\n",
      "structural\n",
      "treatment\n",
      "nonlinear\n",
      "beam\n",
      "behaviour\n",
      "----------------------\n",
      "-Topic 15:\n",
      "theorem\n",
      "discrete\n",
      "number\n",
      "linear algebra\n",
      "random\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print_topics_top_words(ldaModel, topics, k, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "$\\alpha = 6.0, \\beta = 1.01, k = 15$\n",
    "\n",
    "Labels for topics:\n",
    "\n",
    "- Topic 1: Thermodynamics\n",
    "- Topic 2: Electronics \n",
    "- Topic 3: Study Methods\n",
    "- Topic 4: Physics\n",
    "- Topic 5: Business\n",
    "- Topic 6: Finance\n",
    "- Topic 7: Business Ventures for AI\n",
    "- Topic 8: Environmental Science\n",
    "- Topic 9: Biology\n",
    "- Topic 10: Medical Research\n",
    "- Topic 11: Robotics\n",
    "- Topic 12: Human Medicine\n",
    "- Topic 13: Materials Engineering\n",
    "- Topic 14: Structure\n",
    "- Topic 15: Maths\n",
    "\n",
    "_Note:_ Some topics are not well defined or hard to defined as they contain words that could be interpreted as severeal topics. Also, the topics are not as clear and separated as desired."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
