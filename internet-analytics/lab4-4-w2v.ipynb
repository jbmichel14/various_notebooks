{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import time\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim_3.8.3/models/keyedvectors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('/ix/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing/utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply different transformations to the string in order to preprocess the data.\n",
    "def process_string(s):\n",
    "    #We remove the punctuation, because (for example) data.the should be two words in the bag of words\n",
    "    s = re.sub(r'\\W',' ',s).split()\n",
    "    #We remove the stopwords, because there are not relevant to the data\n",
    "    s = [word for word in s if word.lower() not in stopwords]\n",
    "    bigrams = [str1+' '+str2 for (str1, str2) in nltk.bigrams(s)]\n",
    "    s.extend(bigrams)\n",
    "    #Filtrer et enlever les bigrams qui apparaissent trop peu souvent\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns normalized vector\n",
    "def normalize_vec(vec):\n",
    "    norm = np.linalg.norm(vec)\n",
    "    result = []\n",
    "    for v in vec:\n",
    "        result.append(v/norm)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dict with words of wl mapped to their occurence\n",
    "def word_list_to_counts(wl):\n",
    "    result = defaultdict(int)\n",
    "    for w in wl:\n",
    "        result[w] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates dict d with number occurences\n",
    "def number_of_occurences_in_list(l, d):\n",
    "    for a in l:\n",
    "       d[a] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes cosine similarity between two vectors\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes Euclidean distance between two vectors\n",
    "def euclidean_dist(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_word_counts = defaultdict(int)\n",
    "for course in courses:\n",
    "    words = process_string(course['description'])\n",
    "    total_word_counts = number_of_occurences_in_list(words, total_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this filter: faster kmeans, but without slower search\n",
    "total_word_counts = {key:val for key, val in total_word_counts.items() if val >= 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataset for kmeans (4.12)\n",
    "dataset = {}\n",
    "\n",
    "#The corpus for the search function (4.13)\n",
    "doc_corpus = []\n",
    "\n",
    "for course in courses:\n",
    "    doc_name = course['name']\n",
    "    description = course['description']\n",
    "    pre_processed_data = process_string(description)\n",
    "    \n",
    "    counts = word_list_to_counts(pre_processed_data) #word counts for this document\n",
    "    #maps a word its word2vec in this document\n",
    "    word2vecs = {} \n",
    "    #number of words in this document\n",
    "    tot_words = 0\n",
    "    for word in pre_processed_data:\n",
    "        if word in word_vectors.vocab:\n",
    "            v = normalize_vec(word_vectors.get_vector(word))\n",
    "            if word in total_word_counts:\n",
    "                dataset[word] = v\n",
    "            word2vecs[word] = v\n",
    "        else:\n",
    "            #all unknown words same vector: all zeroes except a one\n",
    "            v = np.concatenate((np.zeros(word_vectors.vector_size - 1), np.ones(1)))\n",
    "            if word in total_word_counts:\n",
    "                dataset[word] = v  \n",
    "            word2vecs[word] = v\n",
    "        tot_words += 1\n",
    "    doc_corpus.append({'doc_name': doc_name, 'word2vecs': word2vecs, 'counts': counts, 'total': tot_words})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "- For all _unknown_ words, we choose the same default vector. Having all zero's except a 1 in one place allows us to have all _uknown_ words clustered in one place, and very different from most other vectors so it doesn't have too much impact on the search function.\n",
    "\n",
    "- For the dataset used for exercise 4.12, it was decided to filter out words that appear only once for performance reasons. But we kept these words for parts 4.13 and 4.14 as they can be important for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input of KMeans\n",
    "X = []\n",
    "for w in dataset.items():\n",
    "    X.append(w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose k = 15 according to the k chosen in execise 4.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "kmeans.fit_predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_top_words_cluster_k(k, centers, labels):\n",
    "    center = centers[k]\n",
    "    indices_of_members = [] #members of center k\n",
    "    \n",
    "    for i in range(0,len(labels)):\n",
    "        if labels[i] == k:\n",
    "            indices_of_members.append(i)\n",
    "            \n",
    "    members = [list(dataset.items())[j] for j in indices_of_members]\n",
    "    \n",
    "    cosine_similarities = [cosine_sim(center, w[1]) for w in members]\n",
    "    #euclidean_distances = [euclidean_dist(center, w[1]) for w in members]\n",
    "    \n",
    "    indices = np.flip(np.argsort(cosine_similarities))\n",
    "    #indices = np.argsort(euclidean_distances)\n",
    "    print(\"---Cluster \" + str(k) + \"---\")\n",
    "    for j in range(0,10):\n",
    "        print(list(dataset.items())[indices_of_members[indices[j]]][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Cluster 0---\n",
      "localization bacteria\n",
      "professionals operate\n",
      "approach Plan\n",
      "disciplinary approach\n",
      "multi disciplinary\n",
      "involve multi\n",
      "learning involve\n",
      "research understanding\n",
      "sociological research\n",
      "draws sociological\n",
      "---Cluster 1---\n",
      "photoluminescence\n",
      "nanostructure\n",
      "dielectric\n",
      "pyroelectric\n",
      "dopants\n",
      "piezoelectricity\n",
      "conductivity\n",
      "photodetectors\n",
      "ionization\n",
      "adsorbate\n",
      "---Cluster 2---\n",
      "Context\n",
      "Perspective\n",
      "Transformation\n",
      "Evaluate\n",
      "Practice\n",
      "Knowledge\n",
      "Theory\n",
      "Influence\n",
      "Mind\n",
      "Concepts\n",
      "---Cluster 3---\n",
      "Optimization\n",
      "Measurement\n",
      "Applications\n",
      "Multiscale\n",
      "Automation\n",
      "Processes\n",
      "Systems\n",
      "Imaging\n",
      "Resistive\n",
      "Analysis\n",
      "---Cluster 4---\n",
      "proteins\n",
      "enzymatic\n",
      "metabolism\n",
      "inhibition\n",
      "metabolic\n",
      "biochemical\n",
      "intracellular\n",
      "neuronal\n",
      "vitro\n",
      "bacterial\n",
      "---Cluster 5---\n",
      "Mathematics\n",
      "Psychology\n",
      "Physics\n",
      "Biochemistry\n",
      "Applied\n",
      "Economics\n",
      "Microbiology\n",
      "Interdisciplinary\n",
      "Philosophy\n",
      "Chemistry\n",
      "---Cluster 6---\n",
      "enable\n",
      "ensure\n",
      "desired\n",
      "interfere\n",
      "needed\n",
      "demonstrate\n",
      "necessity\n",
      "affect\n",
      "fail\n",
      "helpful\n",
      "---Cluster 7---\n",
      "thin\n",
      "vertical\n",
      "thick\n",
      "circular\n",
      "horizontal\n",
      "cylindrical\n",
      "curved\n",
      "concave\n",
      "shallow\n",
      "shape\n",
      "---Cluster 8---\n",
      "Baker\n",
      "Smith\n",
      "Robinson\n",
      "Taylor\n",
      "Meyer\n",
      "Freeman\n",
      "Cummings\n",
      "Patterson\n",
      "Evans\n",
      "Cohen\n",
      "---Cluster 9---\n",
      "210\n",
      "310\n",
      "213\n",
      "232\n",
      "236\n",
      "205\n",
      "207\n",
      "203\n",
      "201\n",
      "208\n",
      "---Cluster 10---\n",
      "ODEs\n",
      "quadratic\n",
      "discretized\n",
      "equations\n",
      "stationarity\n",
      "summability\n",
      "finite\n",
      "discretization\n",
      "invariant\n",
      "univariate\n",
      "---Cluster 11---\n",
      "interfaces\n",
      "software\n",
      "implementations\n",
      "interface\n",
      "Simulink\n",
      "functionality\n",
      "optimized\n",
      "APIs\n",
      "rasterization\n",
      "scalable\n",
      "---Cluster 12---\n",
      "investments\n",
      "stakeholders\n",
      "infrastructure\n",
      "incentives\n",
      "implementing\n",
      "sustainability\n",
      "governmental\n",
      "economic\n",
      "initiatives\n",
      "institutional\n",
      "---Cluster 13---\n",
      "2009\n",
      "2007\n",
      "2008\n",
      "2005\n",
      "2012\n",
      "2003\n",
      "2013\n",
      "June\n",
      "1999\n",
      "2004\n",
      "---Cluster 14---\n",
      "context\n",
      "concepts\n",
      "notion\n",
      "interpretation\n",
      "notions\n",
      "emphasizing\n",
      "understanding\n",
      "relevance\n",
      "perspectives\n",
      "implications\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print_top_words_cluster_k(i, centers, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: \n",
    "\n",
    "Cluster **0** contains all the 'unknown' words.\n",
    "In general, we observe that k-means clusters the words by type of word rather than by subject, but some clusters, like cluster **4** and **11**, still represent a specific topic.\n",
    "We observe several types of clusters. Some contain names of scientific fields, like cluster **3**. Cluster **8** contains quite a lot of first names. Cluster **9** only contains numbers. Clusters **6** and. **14** contain quite random words.\n",
    "\n",
    "The labels for 10 clusters (excluding cluster 0):\n",
    "\n",
    "1. Cluster 1: Advanced scientific terms\n",
    "2. Cluster 2: General academic terms\n",
    "3. Cluster 3: General scientific terms\n",
    "4. Cluster 4: Biology\n",
    "5. Cluster 5: Taught subjects\n",
    "6. Cluster 7: Adjectives that describe shapes\n",
    "7. Cluster 8: First names\n",
    "8. Cluster 11: Computer science\n",
    "9. Cluster 12: Finance\n",
    "10. Cluster 13: Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to LSI and LDA, k-means on Word2Vec vectors generates quite a good clustering of words, but doesn't separate topics (as the definition of a topic) as well as LDA and LSI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more pre-processing and TF-IDF computation before we can implement a search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(word_counts, total_doc):\n",
    "    result = {}\n",
    "    for word, count in word_counts.items():\n",
    "        result[word] = count / float(total_doc)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf\n",
    "idf = defaultdict(int)\n",
    "N = len(doc_corpus)\n",
    "for doc in doc_corpus:\n",
    "    for word, count in doc['counts'].items():\n",
    "        if count > 0:\n",
    "            idf[word] += 1\n",
    "for word, count in idf.items():\n",
    "    idf[word] = np.log(N/float(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(word_counts, total_doc, idf):\n",
    "    tf_idf = {}\n",
    "    tf = compute_tf(word_counts, total_doc)\n",
    "    for word, val in tf.items():\n",
    "        tf_idf[word] = val*idf[word]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_corpus = []\n",
    "for doc in doc_corpus:\n",
    "    tf_idf = compute_tf_idf(doc['counts'], doc['total'], idf)\n",
    "    processed_corpus.append({'doc_name': doc['doc_name'], 'word2vecs': doc['word2vecs'], 'tf_idf': tf_idf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of {doc_name: weight}    \n",
    "doc_averages = {}\n",
    "for doc in processed_corpus:\n",
    "    name = doc['doc_name']\n",
    "    somme = 0\n",
    "    for word,vec in doc['word2vecs'].items():\n",
    "        weight = doc['tf_idf'][word]\n",
    "        somme += weight*np.array(vec)\n",
    "    doc_averages[name] = somme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns similarity score for each document (the query has to be our the dataset)\n",
    "def search(string):\n",
    "    start = time.time()\n",
    "    #similarity score is cosine_similarity between the document's weighted average and the word's representation\n",
    "    if string not in dataset:\n",
    "        print(\"Your search - \" + string + \" - did not match any documents.\")\n",
    "        return\n",
    "    vec = dataset[string]\n",
    "    avgs = list(doc_averages.items())\n",
    "    sims = [cosine_sim(vec, w[1]) for w in avgs]\n",
    "    r = np.flip(np.argsort(sims))\n",
    "    print(string + ':')\n",
    "    for i in range(5):\n",
    "        print(avgs[r[i]][0] + ' (score: ' + str(sims[r[i]]) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov chains:\n",
      "Project in Biotechnology (score: 0.9926106825631218)\n",
      "Optics I (score: 0.9925580402560084)\n",
      "Mathematical modelling of behavior (score: 0.9885238787107079)\n",
      "Optics II (score: 0.9881659074790364)\n",
      "Concurrent algorithms (score: 0.9879251764226877)\n"
     ]
    }
   ],
   "source": [
    "search('Markov chains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook:\n",
      "Computational Social Media (score: 0.12890162)\n",
      "Social media (score: 0.067226306)\n",
      "Privacy Protection (score: 0.06489868)\n",
      "Fundamentals of Biometrics (score: 0.064659774)\n",
      "Computer networks (score: 0.064403765)\n"
     ]
    }
   ],
   "source": [
    "search('Facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam:\n",
      "Training Rotation (EDNE) (score: 0.18543461)\n",
      "Project 2 (EDIC) (score: 0.17522454)\n",
      "Project 1 (EDIC) (score: 0.17522454)\n",
      "Field Research Project A (score: 0.11598104)\n",
      "Field Research Project B (score: 0.11155017)\n"
     ]
    }
   ],
   "source": [
    "search('exam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: \n",
    "\n",
    "The search function gives a relatively relevant result most of the time, giving courses that seem to have something to do with the given word (especially for 'Facebook'). Of course, the result is relevant as long as we ask for quite precise words. For example, searching for 'exam' does not make much sense. In some ways it is inconsistent, for example when giving the same word with the first letter uppercase and lowercase, the result can be quite different.\n",
    "\n",
    "\n",
    "The results found for 'Markov chains' seem less relevant than the results using Vector Space Models (which had spot-on results of courses that are explicitely about Markov chains) or LSI that generated more implicitely relevant results. \n",
    "\n",
    "VSM had only 1 result for 'Facebook' which was the same as our first result. And compared to LSI, the results found here seem more relevant and linked to the searched word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_general(string):\n",
    "    if string not in dataset:\n",
    "        if string not in word_vectors:\n",
    "            print(\"Your search - \" + string + \" - did not match any documents.\")\n",
    "            return\n",
    "        vec = normalize_vec(word_vectors[string])\n",
    "        avgs = list(doc_averages.items())\n",
    "        sims = [cosine_sim(vec, w[1]) for w in avgs]\n",
    "        r = np.flip(np.argsort(sims))\n",
    "        print(string + ':')\n",
    "        for i in range(0,5):\n",
    "            print(avgs[r[i]][0] + \" (score: \" + str(sims[r[i]]) + ')')\n",
    "    else:\n",
    "        print(\"The query was found in our dataset\")\n",
    "        search(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: from a coding perspective, we can easily combine the two searches to have more elegant code, but for clarity of the exercise we keep them separate._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySpace:\n",
      "Computational Social Media (score: 0.10054466)\n",
      "Training Rotation (EDNE) (score: 0.09367811)\n",
      "Computer networks (score: 0.052398767)\n",
      "Satellite communications  systems and networks (score: 0.050787453)\n",
      "Social media (score: 0.048894677)\n"
     ]
    }
   ],
   "source": [
    "search_general('MySpace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orkut:\n",
      "Computational Social Media (score: 0.16581577)\n",
      "Computer networks (score: 0.1276489)\n",
      "Social media (score: 0.124385275)\n",
      "Satellite communications  systems and networks (score: 0.122522555)\n",
      "Privacy Protection (score: 0.11618064)\n"
     ]
    }
   ],
   "source": [
    "search_general('Orkut')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: \n",
    "\n",
    "The results for 'MySpace' and 'Orkut', which are both old social networking service, are pretty relevant, although for MySpace _Social Media_ only comes at fifth place, and _Satellite communications systems and networks_ tends to come pretty high. Compared to 'Facebook', both come satisfyingly close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus:\n",
      "Training Rotation (EDNE) (score: 0.12073704)\n",
      "Chemosensory receptors: Applications for biosensors and medical therapies (score: 0.11710419)\n",
      "Infection biology (score: 0.108508505)\n",
      "Chemistry of small biological molecules (score: 0.104032196)\n",
      "Practical - Brisken Lab (score: 0.10148764)\n"
     ]
    }
   ],
   "source": [
    "search_general('coronavirus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: \n",
    "\n",
    "Again, the results are good for 'coronavirus', as we obtain biology-related results (_Brisken Lab_ is also in the BIO faculty).\n",
    "\n",
    "Remark: For some reason, _Training Rotation (EDNE)_ seems to appear in the top 5 results quite often. This may be explained by the fact that its course description only contains 2 words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
